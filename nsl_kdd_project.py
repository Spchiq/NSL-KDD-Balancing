# -*- coding: utf-8 -*-
"""NSL-KDD Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V7KYnka8Z4GhefGIw23quQA407rKTY9j

#**Importing Libraries**

Here we imported all the necessary libraries
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install ctgan
# %pip install dill

from __future__ import absolute_import, division, print_function, unicode_literals

import os
import time
import pandas as pd
import numpy as np
from sklearn import metrics

from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler

import cv2
import seaborn as sns
import matplotlib.pyplot as plt

"""#**Importing all Data**

Here we imported the NSL-DD datasets and added column labels to it.
"""

#Connecting Google Colab with Google Drive

from google.colab import drive
drive.mount('/content/drive')

#Defining those files

file_path_20_percent  = '/content/drive/My Drive/Colab Notebooks/Colab Data/nsl-kdd/KDDTrain+_20Percent.txt'
file_path_full_training_set = '/content/drive/My Drive/Colab Notebooks/Colab Data/nsl-kdd/KDDTrain+.txt'
file_path_test = '/content/drive/My Drive/Colab Notebooks/Colab Data/nsl-kdd/KDDTest+.txt'

#df = pd.read_csv(file_path_20_percent)
df = pd.read_csv(file_path_full_training_set)
test_df = pd.read_csv(file_path_test)

# add the column labels
columns = (['duration'
,'protocol_type'
,'service'
,'flag'
,'src_bytes'
,'dst_bytes'
,'land'
,'wrong_fragment'
,'urgent'
,'hot'
,'num_failed_logins'
,'logged_in'
,'num_compromised'
,'root_shell'
,'su_attempted'
,'num_root'
,'num_file_creations'
,'num_shells'
,'num_access_files'
,'num_outbound_cmds'
,'is_host_login'
,'is_guest_login'
,'count'
,'srv_count'
,'serror_rate'
,'srv_serror_rate'
,'rerror_rate'
,'srv_rerror_rate'
,'same_srv_rate'
,'diff_srv_rate'
,'srv_diff_host_rate'
,'dst_host_count'
,'dst_host_srv_count'
,'dst_host_same_srv_rate'
,'dst_host_diff_srv_rate'
,'dst_host_same_src_port_rate'
,'dst_host_srv_diff_host_rate'
,'dst_host_serror_rate'
,'dst_host_srv_serror_rate'
,'dst_host_rerror_rate'
,'dst_host_srv_rerror_rate'
,'attack'
,'level'])

df.columns = columns
test_df.columns = columns

# sanity check
df.head()

"""#**Data Transformations**

1. We create a new column with a binary value that represents if the access is an attack or not.
2. Create lists with labels for our attack classifications and attack labels. These lists will help with plotting and result visualization.
3. Create a table that crosses the types of attacks with the types of access (protocols).
"""

# map normal to 0, all attacks to 1
is_attack = df.attack.map(lambda a: 0 if a == 'normal' else 1)
test_attack = test_df.attack.map(lambda a: 0 if a == 'normal' else 1)

#data_with_attack = df.join(is_attack, rsuffix='_flag')
df['attack_flag'] = is_attack
test_df['attack_flag'] = test_attack

# view the result
df.head()

# lists to hold our attack classifications
dos_attacks = ['apache2','back','land','neptune','mailbomb','pod','processtable','smurf','teardrop','udpstorm','worm']
probe_attacks = ['ipsweep','mscan','nmap','portsweep','saint','satan']
privilege_attacks = ['buffer_overflow','loadmdoule','perl','ps','rootkit','sqlattack','xterm']
access_attacks = ['ftp_write','guess_passwd','http_tunnel','imap','multihop','named','phf','sendmail','snmpgetattack','snmpguess','spy','warezclient','warezmaster','xclock','xsnoop']

# we will use these for plotting 
attack_labels = ['Normal','DoS','Probe','Privilege','Access']

# helper function to pass to data frame mapping
def map_attack(attack):
    if attack in dos_attacks:
        # dos_attacks map to 1
        attack_type = 1
    elif attack in probe_attacks:
        # probe_attacks mapt to 2
        attack_type = 2
    elif attack in privilege_attacks:
        # privilege escalation attacks map to 3
        attack_type = 3
    elif attack in access_attacks:
        # remote access attacks map to 4
        attack_type = 4
    else:
        # normal maps to 0
        attack_type = 0
        
    return attack_type

# map the data and join to the data set
attack_map = df.attack.apply(map_attack)
df['attack_map'] = attack_map

test_attack_map = test_df.attack.apply(map_attack)
test_df['attack_map'] = test_attack_map

# view the result
df.head()

# use a crosstab to get attack vs protocol

attack_vs_protocol = pd.crosstab(df.attack, df.protocol_type)
attack_vs_protocol

from sklearn import preprocessing
le = preprocessing.LabelEncoder()

df['protocol_type'] = le.fit_transform(df['protocol_type'])
df['flag'] = le.fit_transform(df['flag'])


test_df['protocol_type'] = le.fit_transform(test_df['protocol_type'])
test_df['flag'] = le.fit_transform(test_df['flag'])

df = df.drop("attack", axis =1)
df = df.drop("service", axis =1)
train = df

test_df = test_df.drop("attack", axis=1)
test_df = test_df.drop("service", axis =1)
test = test_df

# #These are the features that we will use during our sampling techniques

# train = df[['duration'
# ,'src_bytes'
# ,'dst_bytes'
# ,'land'
# ,'wrong_fragment'
# ,'urgent'
# ,'hot'
# ,'num_failed_logins'
# ,'logged_in'
# ,'num_compromised'
# ,'root_shell'
# ,'su_attempted'
# ,'num_root'
# ,'num_file_creations'
# ,'num_shells'
# ,'num_access_files'
# ,'num_outbound_cmds'
# ,'is_host_login'
# ,'is_guest_login'
# ,'count'
# ,'srv_count'
# ,'serror_rate'
# ,'srv_serror_rate'
# ,'rerror_rate'
# ,'srv_rerror_rate'
# ,'same_srv_rate'
# ,'diff_srv_rate'
# ,'srv_diff_host_rate'
# ,'dst_host_count'
# ,'dst_host_srv_count'
# ,'dst_host_same_srv_rate'
# ,'dst_host_diff_srv_rate'
# ,'dst_host_same_src_port_rate'
# ,'dst_host_srv_diff_host_rate'
# ,'dst_host_serror_rate'
# ,'dst_host_srv_serror_rate'
# ,'dst_host_rerror_rate'
# ,'dst_host_srv_rerror_rate'
# ,'level'
# ,'attack_map'
# ,'attack_flag']].copy()

# #x = train
# #y = train['attack_map']


# test = test_df[['duration'
# ,'src_bytes'
# ,'dst_bytes'
# ,'land'
# ,'wrong_fragment'
# ,'urgent'
# ,'hot'
# ,'num_failed_logins'
# ,'logged_in'
# ,'num_compromised'
# ,'root_shell'
# ,'su_attempted'
# ,'num_root'
# ,'num_file_creations'
# ,'num_shells'
# ,'num_access_files'
# ,'num_outbound_cmds'
# ,'is_host_login'
# ,'is_guest_login'
# ,'count'
# ,'srv_count'
# ,'serror_rate'
# ,'srv_serror_rate'
# ,'rerror_rate'
# ,'srv_rerror_rate'
# ,'same_srv_rate'
# ,'diff_srv_rate'
# ,'srv_diff_host_rate'
# ,'dst_host_count'
# ,'dst_host_srv_count'
# ,'dst_host_same_srv_rate'
# ,'dst_host_diff_srv_rate'
# ,'dst_host_same_src_port_rate'
# ,'dst_host_srv_diff_host_rate'
# ,'dst_host_serror_rate'
# ,'dst_host_srv_serror_rate'
# ,'dst_host_rerror_rate'
# ,'dst_host_srv_rerror_rate'
# ,'level'
# ,'attack_map'
# ,'attack_flag']].copy()

# #x_test = test
# #y_test = test['attack_map']

#Normalizing Data before use

from sklearn.preprocessing import MinMaxScaler

x_map = train['attack_map']
x_test_map = test['attack_map']

train = train.drop(['attack_map'], axis = 1)
test = test.drop(['attack_map'], axis = 1)

scaler = MinMaxScaler()
scaler.fit(train)
scaled = scaler.fit_transform(train)
scaled_df = pd.DataFrame(scaled, columns = train.columns)
train = scaled_df
train = pd.concat([train, x_map], axis = 1)

x = train
y = train['attack_map']

scaler2 = MinMaxScaler()
scaler2.fit(test)
scaled2 = scaler.fit_transform(test)
scaled_df2 = pd.DataFrame(scaled2, columns = test.columns)
test = scaled_df2
test = pd.concat([test, x_test_map], axis = 1)

x_test = test
y_test = test['attack_map']

"""#**Random OverSampling**

"""

pd.Series(y).value_counts().plot( kind = 'bar', title = 'Distribution Original DataSet')

# Taking the Majority Class ('0') column from the dataset
x_over = x[x.attack_map != 0]
x_append = x[x.attack_map == 0]

y_over = y[y != 0]
y_append = y[y == 0]

# Performing Oversampling

over = RandomOverSampler()
df_Over, y_under = over.fit_resample(x_over, y_over)

pd.Series(y_under).value_counts().plot( kind = 'bar', title = 'Distribution after Random Oversampling without majority Class')

# ReAdding the Majority Class

df_Over = df_Over.append(x_append)
y_under = y_under.append(y_append)

pd.Series(y_under).value_counts().plot( kind = 'bar', title = 'Distribution after appending Majority Class')

# UnderSampling Majority class to match other classes

under = RandomUnderSampler()
x_Random, y_Random = under.fit_resample(df_Over, y_under)

"""#**SMOTE**
1. SMOTE is used to balance the number of entries per class. In this case the class targeted will be attack_map which describes the types of attack.
"""

#Training and Running the SMOTE Balancing Method on Original DataSet (will balance everything according to majority class)
from imblearn.over_sampling import SMOTE 
sm = SMOTE(random_state=42)
x_SMOTE, y_SMOTE = sm.fit_resample(x, y)

#Original Distribution for the types of attacks (From attack_map)
pd.Series(y).value_counts().plot( kind = 'bar', title = 'Distribution Original DataSet')

# Distribution for the types of attacks after SMOTE is implemented for oversampling minorities (From attack_map)
pd.Series(y_SMOTE).value_counts().plot( kind = 'bar', title = 'Distribution after SMOTE')

"""#**SMOTE with UnderSampling**

1.   Elemento de lista
2.   Elemento de lista



"""

pd.Series(y).value_counts().plot( kind = 'bar', title = 'Distribution Original DataSet')

# Saving majority class for later
x_over = x[x.attack_map != 0]
x_append = x[x.attack_map == 0]

y_over = y[y != 0]
y_append = y[y == 0]

#OverSampling until half of the value of the Major Class
df_SMOTE2, y_under = sm.fit_resample(x_over, y_over)

#UnderSampling The bigger Class
#from imblearn.under_sampling import RandomUnderSampler 

#undersample = RandomUnderSampler( sampling_strategy= 'majority', random_state= 42, replacement=False)
#x_under, y_new = undersample.fit_resample(x, y)

pd.Series(y_under).value_counts().plot( kind = 'bar', title = 'Distribution after SMOTE oversampling without majority Class')

# Readding Majority class
df_SMOTE2 = df_SMOTE2.append(x_append)
y_under = y_under.append(y_append)

pd.Series(y_under).value_counts().plot( kind = 'bar', title = 'Distribution after appending Majority Class')

#Now we use under Sampling to bring the majority class down with the others
from imblearn.under_sampling import RandomUnderSampler 

undersample = RandomUnderSampler( sampling_strategy= 'majority', random_state= 42, replacement=False)
x_SMOTE_Under, y_SMOTE_Under = undersample.fit_resample(df_SMOTE2, y_under)

pd.Series(y_SMOTE_Under).value_counts().plot(kind='bar', title = 'Distribution for SMOTE with Random UnderSampling')

"""#**SMOTE with overSampled Minority**"""

pd.Series(y).value_counts().plot( kind = 'bar', title = 'Distribution Original DataSet')

# Mapping each class to a single dataframe (Easier to tamper later)
x_0 = x[x["attack_map"] == 0]
x_1 = x[x["attack_map"] == 1]
x_2 = x[x["attack_map"] == 2]
x_3 = x[x["attack_map"] == 3]
x_4 = x[x["attack_map"] == 4]

#Creating a dataset with the three smaller classes
x_over = x_2.append(x_3)
x_over = x_over.append(x_4)

y_over = x_over['attack_map']

#OverSampling to the largest of the three classes 
over = RandomOverSampler()

df_over, y_over = over.fit_resample(x_over, y_over)

pd.Series(y_over).value_counts().plot( kind = 'bar', title = 'Distribution Original DataSet')

#ReAdding the other two classes
df_over = df_over.append(x_1)
df_over = df_over.append(x_0)
y_over = df_over['attack_map']

pd.Series(y_over).value_counts().plot( kind = 'bar')

# USing SMOTE to match all classes to majority class
from imblearn.over_sampling import SMOTE 
sm = SMOTE(random_state=42)
x_SMOTE_over, y_SMOTE_over = sm.fit_resample(df_over, y_over)

pd.Series(y_SMOTE_over).value_counts().plot( kind = 'bar', title = 'Distribution Original DataSet')

"""
















































































































































































































































































































































































































































































#**CTGAN**
1. We will now use a technique known as Generative Adversarial Networks (GAN) to balance the dataset. We will reuse the already prepped data that was used for SMOTE.
"""

from ctgan import CTGAN
from ctgan import load_demo

pd.Series(y).value_counts().plot( kind = 'bar', title = 'Distribution Original DataSet')

#Creating a dataset from each class
x_0 = x[x["attack_map"] == 0]
x_1 = x[x["attack_map"] == 1]
x_2 = x[x["attack_map"] == 2]
x_3 = x[x["attack_map"] == 3]
x_4 = x[x["attack_map"] == 4]

#CTGAN needs to work

columns = ['duration'
,'src_bytes'
,'dst_bytes'
,'land'
,'wrong_fragment'
,'urgent'
,'hot'
,'num_failed_logins'
,'logged_in'
,'num_compromised'
,'root_shell'
,'su_attempted'
,'num_root'
,'num_file_creations'
,'num_shells'
,'num_access_files'
,'num_outbound_cmds'
,'is_host_login'
,'is_guest_login'
,'count'
,'srv_count'
,'serror_rate'
,'srv_serror_rate'
,'rerror_rate'
,'srv_rerror_rate'
,'same_srv_rate'
,'diff_srv_rate'
,'srv_diff_host_rate'
,'dst_host_count'
,'dst_host_srv_count'
,'dst_host_same_srv_rate'
,'dst_host_diff_srv_rate'
,'dst_host_same_src_port_rate'
,'dst_host_srv_diff_host_rate'
,'dst_host_serror_rate'
,'dst_host_srv_serror_rate'
,'dst_host_rerror_rate'
,'dst_host_srv_rerror_rate'
,'level'
,'attack_map'
,'attack_flag']

x_10 = x_1[:10000]
x_20 = x_2[:10000]
x_30 = x_3[:10000]
x_40 = x_4[:10000]

#Train (10 epochs) / (Using TPU)
#1000 = 32 secs
#2000 = 1.21 min
#3000 = 2.14 mins
#10000 = 13.8 mins

#Train (5 epochs) / (Using TPU)
#3000 = 1.28 min

#Create
#1000(3k) = 0 secs
#35000(10k) = 1 sec
#35000(10k) = 34 secs

#x_0.index = 67351
#x_1.index = 45927
#x_2.index = 11656
#x_3.index = 41
#x_4.index = 995

#Creating Synthetic Data with CTGAN (Will be used to balance that we will later append to the original data to equal the number of entries of major class)
ctgan = CTGAN(epochs = 10)
ctgan.fit(x_10, columns)
synthetic_data1 = ctgan.sample(21424)

ctgan = CTGAN(epochs = 10)
ctgan.fit(x_20, columns)
synthetic_data2 = ctgan.sample(55695)

ctgan = CTGAN(epochs = 10)
ctgan.fit(x_30, columns)
synthetic_data3 = ctgan.sample(67310)

ctgan = CTGAN(epochs = 10)
ctgan.fit(x_40, columns)
synthetic_data4 = ctgan.sample(66536)

#Appending all classes into single dataset
x_CTGAN = x.append(synthetic_data1)
x_CTGAN = x_CTGAN.append(synthetic_data2)
x_CTGAN = x_CTGAN.append(synthetic_data3)
x_CTGAN = x_CTGAN.append(synthetic_data4)
X_CTGAN = x_CTGAN.append(x_0)
x_CTGAN.shape
y_CTGAN=x_CTGAN['attack_map']

pd.Series(y_CTGAN).value_counts().plot( kind = 'bar', title = 'Distribution Original DataSet')

"""#**CTGAN with UnderSampled Major class**

"""

pd.Series(y).value_counts().plot( kind = 'bar', title = 'Distribution Original DataSet')

#Creating subsets with each class
x_0 = x[x["attack_map"] == 0]
x_1 = x[x["attack_map"] == 1]
x_2 = x[x["attack_map"] == 2]
x_3 = x[x["attack_map"] == 3]
x_4 = x[x["attack_map"] == 4]

#CTGAN needs this to work
columns = ['duration'
,'src_bytes'
,'dst_bytes'
,'land'
,'wrong_fragment'
,'urgent'
,'hot'
,'num_failed_logins'
,'logged_in'
,'num_compromised'
,'root_shell'
,'su_attempted'
,'num_root'
,'num_file_creations'
,'num_shells'
,'num_access_files'
,'num_outbound_cmds'
,'is_host_login'
,'is_guest_login'
,'count'
,'srv_count'
,'serror_rate'
,'srv_serror_rate'
,'rerror_rate'
,'srv_rerror_rate'
,'same_srv_rate'
,'diff_srv_rate'
,'srv_diff_host_rate'
,'dst_host_count'
,'dst_host_srv_count'
,'dst_host_same_srv_rate'
,'dst_host_diff_srv_rate'
,'dst_host_same_src_port_rate'
,'dst_host_srv_diff_host_rate'
,'dst_host_serror_rate'
,'dst_host_srv_serror_rate'
,'dst_host_rerror_rate'
,'dst_host_srv_rerror_rate'
,'level'
,'attack_map'
,'attack_flag']

x_10 = x_1[:6000]
x_20 = x_2[:6000]
x_30 = x_3[:6000]
x_40 = x_4[:6000]

#x_0.index = 67351
#x_1.index = 45927
#x_2.index = 11656
#x_3.index = 41
#x_4.index = 995

#Using CTGAN to balance classes with second biggest class
ctgan = CTGAN(epochs = 10)
ctgan.fit(x_20, columns)
synthetic_data2 = ctgan.sample(34271)

ctgan = CTGAN(epochs = 10)
ctgan.fit(x_30, columns)
synthetic_data3 = ctgan.sample(45886)

ctgan = CTGAN(epochs = 10)
ctgan.fit(x_40, columns)
synthetic_data4 = ctgan.sample(45112)

x_2.shape

#Joinig all classes for single dataset
x_CTGAN_Under = x
x_CTGAN_Under = x_CTGAN_Under.append(synthetic_data2)
x_CTGAN_Under = x_CTGAN_Under.append(synthetic_data3)
x_CTGAN_Under = x_CTGAN_Under.append(synthetic_data4)
x_CTGAN_Under.shape
y_CTGAN_Under=x_CTGAN_Under['attack_map']

x_CTGAN_Under.shape

pd.Series(y_CTGAN_Under).value_counts().plot( kind = 'bar', title = 'Distribution Original DataSet')

#Now we use under Sampling to bring the majority class down with the others
from imblearn.under_sampling import RandomUnderSampler 

undersample = RandomUnderSampler( sampling_strategy= 'majority', random_state= 42, replacement=False)
x_CTGAN_Hybrid, y_CTGAN_Hybrid = undersample.fit_resample(x_CTGAN_Under, y_CTGAN_Under)

pd.Series(y_CTGAN_Hybrid).value_counts().plot( kind = 'bar', title = 'Distribution Original DataSet')

"""#**CTGAN w/ OverSampling**"""

pd.Series(y).value_counts().plot( kind = 'bar', title = 'Distribution Original DataSet')

#Mapping class to dataset
x_0 = x[x["attack_map"] == 0]
x_1 = x[x["attack_map"] == 1]
x_2 = x[x["attack_map"] == 2]
x_3 = x[x["attack_map"] == 3]
x_4 = x[x["attack_map"] == 4]

x_over = x_2.append(x_3)
x_over = x_over.append(x_4)

y_over = x_over['attack_map']

#Randomly oversample smaller 2 classes in accordance to third smaller
over = RandomOverSampler()
df_over, y_over = over.fit_resample(x_over, y_over)

pd.Series(y_over).value_counts().plot( kind = 'bar', title = 'Distribution Original DataSet')

x_2 = df_over[df_over["attack_map"] == 2]
x_3 = df_over[df_over["attack_map"] == 3]
x_4 = df_over[df_over["attack_map"] == 4]

x_4.shape

x_10 = x_1[:10000]
x_20 = x_2[:10000]
x_30 = x_3[:10000]
x_40 = x_4[:10000]

#x_0.index = 67351
#x_1.index = 45927
#x_2.index = 11656
#x_3.index = 11656
#x_4.index = 11656

#Balance all classes to majority class using CTGAN
ctgan = CTGAN(epochs = 10)
ctgan.fit(x_10, columns)
synthetic_data1 = ctgan.sample(21424)

ctgan = CTGAN(epochs = 10)
ctgan.fit(x_20, columns)
synthetic_data2 = ctgan.sample(55695)

ctgan = CTGAN(epochs = 10)
ctgan.fit(x_30, columns)
synthetic_data3 = ctgan.sample(55695)

ctgan = CTGAN(epochs = 10)
ctgan.fit(x_40, columns)
synthetic_data4 = ctgan.sample(55695)

#Append all classes together for dataset
x_CTGAN = df_over.append(synthetic_data1)
x_CTGAN = x_CTGAN.append(synthetic_data2)
x_CTGAN = x_CTGAN.append(synthetic_data3)
x_CTGAN = x_CTGAN.append(x_0)
x_CTGAN = x_CTGAN.append(x_1)

x_CTGAN_over = x_CTGAN.append(synthetic_data4)
y_CTGAN_over=x_CTGAN_over['attack_map']

pd.Series(y_CTGAN_over).value_counts().plot( kind = 'bar', title = 'Distribution Original DataSet')

"""#**All Data ready for Training**

"""

from sklearn.utils import shuffle

#12600 = Full training set
#12600 * .80 = 100800 Train
#12600 * .2  = 25000  Verify

#We have to shuffle the balanced
x_SMOTE_Under_temp = shuffle(x_SMOTE_Under)
x_SMOTE_Under = x_SMOTE_Under_temp
x_SMOTE_Under = x_SMOTE_Under.drop(['attack_map'], axis = 1)
y_SMOTE_Under = x_SMOTE_Under_temp['attack_map']

x_SMOTE_temp = shuffle(x_SMOTE)
x_SMOTE = x_SMOTE_temp
x_SMOTE = x_SMOTE.drop(['attack_map'], axis = 1)
y_SMOTE = x_SMOTE_temp['attack_map']

x_SMOTE_over_temp = shuffle(x_SMOTE_over)
x_SMOTE_over = x_SMOTE_over_temp
x_SMOTE_over = x_SMOTE_over.drop(['attack_map'], axis = 1)
y_SMOTE_over = x_SMOTE_over_temp['attack_map']

x_Random_temp = shuffle(x_Random)
x_Random = x_Random_temp
x_Random = x_Random.drop(['attack_map'], axis = 1)
y_Random = x_Random_temp['attack_map']

x_CTGAN_temp = shuffle(x_CTGAN)
x_CTGAN = x_CTGAN_temp
x_CTGAN = x_CTGAN.drop(['attack_map'], axis = 1)
y_CTGAN = x_CTGAN_temp['attack_map']

x_CTGAN_Hybrid_temp = shuffle(x_CTGAN_Hybrid)
x_CTGAN_Hybrid = x_CTGAN_Hybrid_temp
x_CTGAN_Hybrid = x_CTGAN_Hybrid.drop(['attack_map'], axis = 1)
y_CTGAN_Hybrid = x_CTGAN_Hybrid_temp['attack_map']

x_CTGAN_over_temp = shuffle(x_CTGAN_over)
x_CTGAN_over = x_CTGAN_over_temp
x_CTGAN_over = x_CTGAN_over.drop(['attack_map'], axis = 1)
y_CTGAN_over = x_CTGAN_over_temp['attack_map']


# When running with full dataset
x_test = x_test.drop(['attack_map'], axis=1)
x_train = x.drop(['attack_map'], axis = 1)
y_train = x['attack_map']

"""#**Saving and loading Datasets (Pickle)**"""

#Using pickle to store runs of code
import pickle

# pickle.dump([x_SMOTE_Under, y_SMOTE_Under, x_SMOTE, y_SMOTE, x_SMOTE_over, y_SMOTE_over, x_Random, y_Random, x_CTGAN, y_CTGAN, x_CTGAN_Hybrid, x_CTGAN_over, y_CTGAN_over, x_train, y_train], open("dataset.p", "wb"))
#x_SMOTE_Under, y_SMOTE_Under, x_SMOTE, y_SMOTE, x_SMOTE_over, y_SMOTE_over, x_Random, y_Random, x_CTGAN, y_CTGAN, x_CTGAN_Hybrid, x_CTGAN_over, y_CTGAN_over, x_train, y_train = pickle.load(open("dataset.p", "rb"))

"""#**Predicting and Comparing using KNN**"""

#Fit model with train data, then create predictions through the x_test dataset
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors = 23).fit(x_train, y_train)
knn_predictions = knn.predict(x_test)

#Confusion Matrix
from sklearn.metrics import confusion_matrix

confusion_matrix = metrics.confusion_matrix(y_test, knn_predictions)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall, Acuracy, Precision, F1 SCore and ROC-AUC Curve

##Recall of positive class is also termed sensitivity and is defined as the ratio of the True Positive to the number of actual positive cases. 
#It can intuitively be expressed as the ability of the classifier to capture all the positive cases. It is also called the True Positive Rate (TPR).
#Out of all positive values, how many are predicted positive.

from IPython.core.interactiveshell import Macro
recall = metrics.recall_score(y_test, knn_predictions, average='macro')

#Accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, knn_predictions)

#Precision
from sklearn.metrics import precision_score
precision = precision_score(y_test, knn_predictions, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, knn_predictions, average = 'macro')

columns = ['Recall', 'Accuracy', 'Precision', 'F1-Score']
scores = pd.DataFrame(columns = columns)
scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]
#scores = [recall, accuracy, precision, f1_score]

knn_Random = KNeighborsClassifier(n_neighbors = 23).fit(x_Random, y_Random)
knn_Random_predictions = knn_Random.predict(x_test)

confusion_matrix_Random = metrics.confusion_matrix(y_test, knn_Random_predictions)
sns.heatmap(confusion_matrix_Random/np.sum(confusion_matrix_Random), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, knn_Random_predictions, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, knn_Random_predictions)

#Precision
precision = precision_score(y_test, knn_Random_predictions, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, knn_Random_predictions, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

knn_SMOTE = KNeighborsClassifier(n_neighbors = 23).fit(x_SMOTE, y_SMOTE)
knn_SMOTE_predictions = knn_SMOTE.predict(x_test)

confusion_matrix_SMOTE = metrics.confusion_matrix(y_test, knn_SMOTE_predictions)
sns.heatmap(confusion_matrix_SMOTE/np.sum(confusion_matrix_SMOTE), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, knn_SMOTE_predictions, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, knn_SMOTE_predictions)

#Precision
precision = precision_score(y_test, knn_SMOTE_predictions, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, knn_SMOTE_predictions, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

knn_Under = KNeighborsClassifier(n_neighbors = 23).fit(x_SMOTE_Under, y_SMOTE_Under)
knn_SMOTE_Under_predictions = knn_Under.predict(x_test)

confusion_matrix_SMOTE_Under = metrics.confusion_matrix(y_test, knn_SMOTE_Under_predictions)
sns.heatmap(confusion_matrix_SMOTE_Under/np.sum(confusion_matrix_SMOTE_Under), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, knn_SMOTE_Under_predictions, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, knn_SMOTE_Under_predictions)

#Precision
precision = precision_score(y_test, knn_SMOTE_Under_predictions, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, knn_SMOTE_Under_predictions, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors = 23).fit(x_SMOTE_over, y_SMOTE_over)
knn_SMOTE_over_predictions = knn.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, knn_SMOTE_over_predictions)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

recall = metrics.recall_score(y_test, knn_SMOTE_over_predictions, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, knn_SMOTE_over_predictions)

#Precision
precision = precision_score(y_test, knn_SMOTE_over_predictions, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, knn_SMOTE_over_predictions, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix

knn_CTGAN = KNeighborsClassifier(n_neighbors = 23).fit(x_CTGAN, y_CTGAN)
knn_CTGAN_predictions = knn_CTGAN.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, knn_CTGAN_predictions)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

recall = metrics.recall_score(y_test, knn_CTGAN_predictions, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, knn_CTGAN_predictions)

#Precision
precision = precision_score(y_test, knn_CTGAN_predictions, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, knn_CTGAN_predictions, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

from sklearn.utils import shuffle

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix

knn_CTGAN_Hybrid = KNeighborsClassifier(n_neighbors = 23).fit(x_CTGAN_Hybrid, y_CTGAN_Hybrid)
knn_CTGAN_Hybrid_predictions = knn_CTGAN_Hybrid.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, knn_CTGAN_Hybrid_predictions)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

recall = metrics.recall_score(y_test, knn_CTGAN_Hybrid_predictions, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, knn_CTGAN_Hybrid_predictions)

#Precision
precision = precision_score(y_test, knn_CTGAN_Hybrid_predictions, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, knn_CTGAN_Hybrid_predictions, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

from sklearn.utils import shuffle

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix

knn_CTGAN_over = KNeighborsClassifier(n_neighbors = 23).fit(x_CTGAN_over, y_CTGAN_over)
knn_CTGAN_over_predictions = knn_CTGAN_over.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, knn_CTGAN_over_predictions)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

from IPython.core.interactiveshell import Macro
recall = metrics.recall_score(y_test, knn_CTGAN_over_predictions, average='macro')

#Accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, knn_CTGAN_over_predictions)

#Precision
from sklearn.metrics import precision_score
precision = precision_score(y_test, knn_CTGAN_over_predictions, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, knn_CTGAN_over_predictions, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

scoresknn = scores

scoresknn

"""#**Support Vector Machine**"""

from sklearn import svm
from sklearn.metrics import confusion_matrix
from IPython.core.interactiveshell import Macro

clf = svm.SVC(kernel='linear')
clf.fit(x_train, y_train)

clf_pred = clf.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, clf_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

print("Accuracy:", metrics.accuracy_score(y_test, clf_pred))

from IPython.core.interactiveshell import Macro
recall = metrics.recall_score(y_test, clf_pred, average='macro')

#Accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, clf_pred)

#Precision
from sklearn.metrics import precision_score
precision = precision_score(y_test, clf_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, clf_pred, average = 'macro')

columns = ['Recall', 'Accuracy', 'Precision', 'F1-Score']
scores = pd.DataFrame(columns = columns)
scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]
#scores = [recall, accuracy, precision, f1_score]

clf = svm.SVC(kernel='linear')
clf.fit(x_Random, y_Random)

clf_Random_pred = clf.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, clf_Random_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, clf_Random_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, clf_Random_pred)

#Precision
precision = precision_score(y_test, clf_Random_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, clf_Random_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

clf = svm.SVC(kernel='linear')
clf.fit(x_SMOTE, y_SMOTE)

clf_SMOTE_pred = clf.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, clf_SMOTE_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, clf_SMOTE_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, clf_SMOTE_pred)

#Precision
precision = precision_score(y_test, clf_SMOTE_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, clf_SMOTE_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

clf = svm.SVC(kernel='linear')
clf.fit(x_SMOTE_Under, y_SMOTE_Under)

clf_SMOTE_Under_pred = clf.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, clf_SMOTE_Under_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, clf_SMOTE_Under_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, clf_SMOTE_Under_pred)

#Precision
precision = precision_score(y_test, clf_SMOTE_Under_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, clf_SMOTE_Under_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

clf = svm.SVC(kernel='linear')
clf.fit(x_SMOTE_over, y_SMOTE_over)

clf_SMOTE_over_pred = clf.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, clf_SMOTE_over_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, clf_SMOTE_over_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, clf_SMOTE_over_pred)

#Precision
precision = precision_score(y_test, clf_SMOTE_over_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, clf_SMOTE_over_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

clf = svm.SVC(kernel='linear')
clf.fit(x_CTGAN, y_CTGAN)

clf_CTGAN_pred = clf.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, clf_CTGAN_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, clf_CTGAN_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, clf_CTGAN_pred)

#Precision
precision = precision_score(y_test, clf_CTGAN_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, clf_CTGAN_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

clf = svm.SVC(kernel='linear')
clf.fit(x_CTGAN_Hybrid, y_CTGAN_Hybrid)

clf_CTGAN_Hybrid_pred = clf.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, clf_CTGAN_Hybrid_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, clf_CTGAN_Hybrid_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, clf_CTGAN_Hybrid_pred)

#Precision
precision = precision_score(y_test, clf_CTGAN_Hybrid_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, clf_CTGAN_Hybrid_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

clf = svm.SVC(kernel='linear')
clf.fit(x_CTGAN_over, y_CTGAN_over)

clf_CTGAN_over_pred = clf.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, clf_CTGAN_over_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, clf_CTGAN_over_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, clf_CTGAN_over_pred)

#Precision
precision = precision_score(y_test, clf_CTGAN_over_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, clf_CTGAN_over_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

scoressvm= scores

scoressvm

"""#**Random Forest**"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators = 100)
rf.fit(x_train, y_train)

rf_pred = rf.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, rf_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

print("Accuracy:", metrics.accuracy_score(y_test, rf_pred))

from IPython.core.interactiveshell import Macro
recall = metrics.recall_score(y_test, rf_pred, average='macro')

#Accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, rf_pred)

#Precision
from sklearn.metrics import precision_score
precision = precision_score(y_test, rf_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, rf_pred, average = 'macro')

columns = ['Recall', 'Accuracy', 'Precision', 'F1-Score']
scores = pd.DataFrame(columns = columns)
scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]
#scores = [recall, accuracy, precision, f1_score]

rf = RandomForestClassifier(n_estimators = 100)
rf.fit(x_Random, y_Random)

rf_Random_pred = rf.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, rf_Random_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, rf_Random_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, rf_Random_pred)

#Precision
precision = precision_score(y_test, rf_Random_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, rf_Random_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

rf = RandomForestClassifier(n_estimators = 100)
rf.fit(x_SMOTE, y_SMOTE)

rf_SMOTE_pred = rf.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, rf_SMOTE_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, rf_SMOTE_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, rf_SMOTE_pred)

#Precision
precision = precision_score(y_test, rf_SMOTE_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, rf_SMOTE_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

rf = RandomForestClassifier(n_estimators = 100)
rf.fit(x_SMOTE_Under, y_SMOTE_Under)

rf_SMOTE_Under_pred = rf.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, rf_SMOTE_Under_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, rf_SMOTE_Under_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, rf_SMOTE_Under_pred)

#Precision
precision = precision_score(y_test, rf_SMOTE_Under_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, rf_SMOTE_Under_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

rf = RandomForestClassifier(n_estimators = 100)
rf.fit(x_SMOTE_over, y_SMOTE_over)

rf_SMOTE_over_pred = rf.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, rf_SMOTE_over_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, rf_SMOTE_over_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, rf_SMOTE_over_pred)

#Precision
precision = precision_score(y_test, rf_SMOTE_over_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, rf_SMOTE_over_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

rf = RandomForestClassifier(n_estimators = 100)
rf.fit(x_CTGAN, y_CTGAN)

rf_CTGAN_pred = rf.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, rf_CTGAN_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, rf_CTGAN_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, rf_CTGAN_pred)

#Precision
precision = precision_score(y_test, rf_CTGAN_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, rf_CTGAN_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

rf = RandomForestClassifier(n_estimators = 100)
rf.fit(x_CTGAN_Hybrid, y_CTGAN_Hybrid)

rf_CTGAN_Hybrid_pred = rf.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, rf_CTGAN_Hybrid_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, rf_CTGAN_Hybrid_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, rf_CTGAN_Hybrid_pred)

#Precision
precision = precision_score(y_test, rf_CTGAN_Hybrid_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, rf_CTGAN_Hybrid_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

rf = RandomForestClassifier(n_estimators = 100)
rf.fit(x_CTGAN_over, y_CTGAN_over)

rf_CTGAN_over_pred = rf.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, rf_CTGAN_over_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, rf_CTGAN_over_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, rf_CTGAN_over_pred)

#Precision
precision = precision_score(y_test, rf_CTGAN_over_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, rf_CTGAN_over_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

scoresrf = scores

scoresrf

"""#**Naive Bayes**"""

from sklearn.naive_bayes import GaussianNB
nb = GaussianNB()
nb.fit(x_train, y_train)

nb_pred = nb.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, nb_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

from IPython.core.interactiveshell import Macro
recall = metrics.recall_score(y_test, nb_pred, average='macro')

#Accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, nb_pred)

#Precision
from sklearn.metrics import precision_score
precision = precision_score(y_test, nb_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, nb_pred, average = 'macro')

columns = ['Recall', 'Accuracy', 'Precision', 'F1-Score']
scores = pd.DataFrame(columns = columns)
scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]
#scores = [recall, accuracy, precision, f1_score]

nb = GaussianNB()
nb.fit(x_Random, y_Random)

nb_Random_pred = nb.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, nb_Random_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, nb_Random_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, nb_Random_pred)

#Precision
precision = precision_score(y_test, nb_Random_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, nb_Random_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

nb = GaussianNB()
nb.fit(x_SMOTE, y_SMOTE)

nb_SMOTE_pred = nb.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, nb_SMOTE_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, nb_SMOTE_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, nb_SMOTE_pred)

#Precision
precision = precision_score(y_test, nb_SMOTE_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, nb_SMOTE_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

nb = GaussianNB()
nb.fit(x_SMOTE_Under, y_SMOTE_Under)

nb_SMOTE_Under_pred = nb.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, nb_SMOTE_Under_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, nb_SMOTE_Under_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, nb_SMOTE_Under_pred)

#Precision
precision = precision_score(y_test, nb_SMOTE_Under_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, nb_SMOTE_Under_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

nb = GaussianNB()
nb.fit(x_SMOTE_over, y_SMOTE_over)

nb_SMOTE_over_pred = nb.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, nb_SMOTE_over_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, nb_SMOTE_over_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, nb_SMOTE_over_pred)

#Precision
precision = precision_score(y_test, nb_SMOTE_over_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, nb_SMOTE_over_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

nb = GaussianNB()
nb.fit(x_CTGAN, y_CTGAN)

nb_CTGAN_pred = nb.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, nb_CTGAN_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, nb_CTGAN_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, nb_CTGAN_pred)

#Precision
precision = precision_score(y_test, nb_CTGAN_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, nb_CTGAN_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

nb = GaussianNB()
nb.fit(x_CTGAN_Hybrid, y_CTGAN_Hybrid)

nb_CTGAN_Hybrid_pred = nb.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, nb_CTGAN_Hybrid_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, nb_CTGAN_Hybrid_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, nb_CTGAN_Hybrid_pred)

#Precision
precision = precision_score(y_test, nb_CTGAN_Hybrid_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, nb_CTGAN_Hybrid_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

nb = GaussianNB()
nb.fit(x_CTGAN_over, y_CTGAN_over)

nb_CTGAN_over_pred = nb.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, nb_CTGAN_over_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, nb_CTGAN_over_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, nb_CTGAN_over_pred)

#Precision
precision = precision_score(y_test, nb_CTGAN_over_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, nb_CTGAN_over_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

scoresnb = scores

scoresnb

"""#**Decision Trees**"""

from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(random_state = 42, max_depth = 5)
dt.fit(x_train, y_train)

dt_pred = dt.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, dt_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

from IPython.core.interactiveshell import Macro
recall = metrics.recall_score(y_test, dt_pred, average='macro')

#Accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, dt_pred)

#Precision
from sklearn.metrics import precision_score
precision = precision_score(y_test, dt_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, dt_pred, average = 'macro')

columns = ['Recall', 'Accuracy', 'Precision', 'F1-Score']
scores = pd.DataFrame(columns = columns)
scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]
#scores = [recall, accuracy, precision, f1_score]

dt = DecisionTreeClassifier(random_state = 42, max_depth = 5)
dt.fit(x_Random, y_Random)

dt_Random_pred = dt.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, dt_Random_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, dt_Random_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, dt_Random_pred)

#Precision
precision = precision_score(y_test, dt_Random_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, dt_Random_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

dt = DecisionTreeClassifier(random_state = 42, max_depth = 5)
dt.fit(x_SMOTE, y_SMOTE)

dt_SMOTE_pred = dt.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, dt_SMOTE_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, dt_SMOTE_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, dt_SMOTE_pred)

#Precision
precision = precision_score(y_test, dt_SMOTE_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, dt_SMOTE_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

dt = DecisionTreeClassifier(random_state = 42, max_depth = 5)
dt.fit(x_SMOTE_Under, y_SMOTE_Under)

dt_SMOTE_Under_pred = dt.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, dt_SMOTE_Under_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, dt_SMOTE_Under_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, dt_SMOTE_Under_pred)

#Precision
precision = precision_score(y_test, dt_SMOTE_Under_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, dt_SMOTE_Under_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

dt = DecisionTreeClassifier(random_state = 42, max_depth = 5)
dt.fit(x_SMOTE_over, y_SMOTE_over)

dt_SMOTE_over_pred = dt.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, dt_SMOTE_over_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, dt_SMOTE_over_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, dt_SMOTE_over_pred)

#Precision
precision = precision_score(y_test, dt_SMOTE_over_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, dt_SMOTE_over_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

dt = DecisionTreeClassifier(random_state = 42, max_depth = 5)
dt.fit(x_CTGAN, y_CTGAN)

dt_CTGAN_pred = dt.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, dt_CTGAN_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, dt_CTGAN_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, dt_CTGAN_pred)

#Precision
precision = precision_score(y_test, dt_CTGAN_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, dt_CTGAN_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

dt = DecisionTreeClassifier(random_state = 42, max_depth = 5)
dt.fit(x_CTGAN_Hybrid, y_CTGAN_Hybrid)

dt_CTGAN_Hybrid_pred = dt.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, dt_CTGAN_Hybrid_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, dt_CTGAN_Hybrid_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, dt_CTGAN_Hybrid_pred)

#Precision
precision = precision_score(y_test, dt_CTGAN_Hybrid_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, dt_CTGAN_Hybrid_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

dt = DecisionTreeClassifier(random_state = 42, max_depth = 5)
dt.fit(x_CTGAN_over, y_CTGAN_over)

dt_CTGAN_over_pred = dt.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, dt_CTGAN_over_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, dt_CTGAN_over_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, dt_CTGAN_over_pred)

#Precision
precision = precision_score(y_test, dt_CTGAN_over_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, dt_CTGAN_over_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

scoresdt = scores

scoresdt

"""#**Extra Trees Classifier**"""

from IPython.utils.frame import extract_vars_above
from sklearn.ensemble import ExtraTreesClassifier
et = ExtraTreesClassifier()
et.fit(x_train, y_train)

et_pred = et.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, dt_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

from IPython.core.interactiveshell import Macro
recall = metrics.recall_score(y_test, et_pred, average='macro')

#Accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, et_pred)

#Precision
from sklearn.metrics import precision_score
precision = precision_score(y_test, et_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, et_pred, average = 'macro')

columns = ['Recall', 'Accuracy', 'Precision', 'F1-Score']
scores = pd.DataFrame(columns = columns)
scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]
#scores = [recall, accuracy, precision, f1_score]

et = ExtraTreesClassifier()
et.fit(x_Random, y_Random)

et_Random_pred = et.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, et_Random_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, et_Random_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, et_Random_pred)

#Precision
precision = precision_score(y_test, et_Random_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, et_Random_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

et = ExtraTreesClassifier()
et.fit(x_SMOTE, y_SMOTE)

et_SMOTE_pred = et.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, et_SMOTE_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, et_SMOTE_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, et_SMOTE_pred)

#Precision
precision = precision_score(y_test, et_SMOTE_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, et_SMOTE_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

et = ExtraTreesClassifier()
et.fit(x_SMOTE_Under, y_SMOTE_Under)

et_SMOTE_Under_pred = et.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, et_SMOTE_Under_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, et_SMOTE_Under_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, et_SMOTE_Under_pred)

#Precision
precision = precision_score(y_test, et_SMOTE_Under_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, et_SMOTE_Under_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

et = ExtraTreesClassifier()
et.fit(x_SMOTE_over, y_SMOTE_over)

et_SMOTE_Over_pred = et.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, et_SMOTE_Over_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, et_SMOTE_Over_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, et_SMOTE_Over_pred)

#Precision
precision = precision_score(y_test, et_SMOTE_Over_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, et_SMOTE_Over_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

et = ExtraTreesClassifier()
et.fit(x_CTGAN, y_CTGAN)

et_CTGAN_pred = et.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, et_CTGAN_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, et_CTGAN_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, et_CTGAN_pred)

#Precision
precision = precision_score(y_test, et_CTGAN_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, et_CTGAN_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

et = ExtraTreesClassifier()
et.fit(x_CTGAN_Hybrid, y_CTGAN_Hybrid)

et_CTGAN_Under_pred = et.predict(x_test)

confusion_matrix = metrics.confusion_matrix(y_test, et_CTGAN_Under_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, et_CTGAN_Under_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, et_CTGAN_Under_pred)

#Precision
precision = precision_score(y_test, et_CTGAN_Under_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, et_CTGAN_Under_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

et = ExtraTreesClassifier()
et.fit(x_CTGAN_over, y_CTGAN_over)

et_CTGAN_Over_pred = et.predict(x_test)

#confusion_matrix = metrics.confusion_matrix(y_test, et_CTGAN_Over_pred)
sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot = True, fmt='.2%', cmap='Blues')

#Recall
recall = metrics.recall_score(y_test, et_CTGAN_Over_pred, average='macro')

#Accuracy
accuracy = accuracy_score(y_test, et_CTGAN_Over_pred)

#Precision
precision = precision_score(y_test, et_CTGAN_Over_pred, average = 'macro')

#F1 Score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, et_CTGAN_Over_pred, average = 'macro')

scores.loc[len(scores.index)] = [recall, accuracy, precision, f1_score]

scoreset = scores

scoreset

"""#**Results**

We will use Recall, Acuracy, Precision, F1 SCore and ROC-AUC Curve

"""

#Adding an Index
index = ['Unbalanced', 'Random Sampling', 'SMOTE', 'SMOTE-Under', 'SMOTE-Over', 'CTGAN', 'CTGAN-Under', 'CTGAN-Over']

scoreset.index = index
scoresrf.index = index
scoresnb.index = index
scoresdt.index = index
scoresknn.index = index
scoressvm.index = index

print("Results for KNN")
print(scoresknn)
print()
print("Results for SVM")
print(scoressvm)
print()
print("Results for Random Forest")
print(scoresrf)
print()
print("Results for Naive Bayes")
print(scoresnb)
print()
print("Results for Decision Trees")
print(scoresdt)
print()
print("Results for Extra Trees Classifier")
print(scoreset)
print()

scoressvm.plot()

#Nested Graph
ax = scores.plot.bar(rot=0)
plt.ylim(.86, 1)

scores.plot(subplots=True)
plt.tight_layout()
plt.show

"""#**Best K-Value for KNN**
Here we looked at the different K-Values and which one will be best for our data.
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import mean_squared_error

#Sqrt of len(x_test.index) = 355

k_values = [*range(1,50,1)]
k_results = []

for x in k_values:
  knn = KNeighborsClassifier(n_neighbors = x).fit(x_train, y_train)
  knn_predictions = knn.predict(x_test)
  mse = mean_squared_error(y_test, knn_predictions)
  k_results.append(mse)

plot = pd.DataFrame(k_results)
plot.plot(title = ')